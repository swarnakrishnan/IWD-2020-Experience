{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of iwd_2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swarnakrishnan/hello-universe/blob/master/Copy_of_iwd_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkSGVSxZDrx8",
        "colab_type": "text"
      },
      "source": [
        "Tips:\n",
        "\n",
        "* Enable a GPU in Colab before running this notebook. *Edit -> Notebook settings -> Hardware accelerator -> GPU.* \n",
        "\n",
        "* Should you need to reset your environment to a clean state, you can use *Runtime -> Factory reset runtime*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2xSeIUHHJ7h",
        "colab_type": "code",
        "outputId": "dc2b1c78-393c-4e99-80b8-f38deccb5f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('hi')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v5mm4amQRrm",
        "colab_type": "text"
      },
      "source": [
        "# IWD 2020: Training Neural Networks with TensorFlow\n",
        "\n",
        "Welcome! Today, you'll gain hands-on experience training neural networks with TensorFlow. This notebook contains several tutorials and exercises. Your instructor will guide you through the sections you'll explore today. \n",
        "\n",
        "If you're new to Deep Learning, this is a *lot* of material to cover in a short workshop. Our goals are to dive in and get started. You'll find educational resources for you to continue learning at the end, and you can complete the sections we don't finish today at home. \n",
        "\n",
        "Here's an outline of what we'll cover.\n",
        "\n",
        "1. You'll train a Deep Neural Network to classify handwritten digits. This is the \"hello world\" of computer vision, and a great place to begin if you're new to the subject. As an exercise, you'll use a different dataset, and modify the network.\n",
        "\n",
        "1. Next, you'll train a Convolutional Neural Network to classify images of cats and dogs, using a real-world dataset you read off disk. As an exercise, you'll use data augmentation and dropout to reduce overfitting.\n",
        "\n",
        "1. If time remains, your instructor will walk you through DeepDream. This is an advanced example that lets you visualize some of the features learned by a CNN.\n",
        "\n",
        "Okay, let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjaqgcQPHufn",
        "colab_type": "text"
      },
      "source": [
        "Welcome to colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_vvkD0hm7ly",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial: MNIST\n",
        "\n",
        "Training an image classifier on the MNIST dataset of handwritten digits is considered the \"hello world\" of computer vision. In this tutorial, you will download the dataset, then train a linear model, a neural network, and a deep neural network to classify it. \n",
        "\n",
        "**Key point:** Deep Learning is \"code light, but concept heavy\". You'll be able to implement a Deep Neural Network in about five lines of code, but the underlying concepts (cross-entropy, softmax, dense layers, etc) normally take a few months to learn. You do need to understand these all today to dive in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06vnmM4FuWOx",
        "colab_type": "text"
      },
      "source": [
        "## Import TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScpqSj42uGjq",
        "colab_type": "text"
      },
      "source": [
        "Let's import TensorFlow. At the time of writing, Colab has TensorFlow version 1.x installed by default. TensorFlow 2.x is much easier to use, so let's start with that. To switch to 2.x we'll use the magic command below. Note, you can also [install](http://tensorflow.org/install) TensorFlow by using `pip`, but in Colab, the magic command is faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jciXPo_3C3Ct",
        "colab_type": "code",
        "outputId": "50428845-ffdd-4caa-fe5a-1d0f08c274c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"You are using TensorFlow version\", tf.__version__)\n",
        "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "  print(\"You have a GPU enabled.\")\n",
        "else:\n",
        "  print(\"Enable a GPU before running this notebook.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You are using TensorFlow version 2.2.0\n",
            "You have a GPU enabled.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2esj4IQFIRN",
        "colab_type": "text"
      },
      "source": [
        "Colab has a variety of GPU types available (each new  instance is assigned one randomly, depending on availability). To see which type of GPU you have, you can run ```!nvidia-smi``` in a code cell. Some are quite fast!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcaimfXjIGb_",
        "colab_type": "code",
        "outputId": "cc179406-238a-4b8e-bd59-dda9d186fca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May 24 10:00:35 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    28W / 149W |     11MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnE-WS8Sn-gA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In this notebook, we'll use Keras: TensorFlow's user-friendly API to \n",
        "# define neural networks. Let's import Keras now.\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBxGBhTAo6tN",
        "colab_type": "text"
      },
      "source": [
        "## Download the MNIST dataset\n",
        "MNIST contains 70,000 grayscale images in 10 categories. The images are low resolution (28 by 28 pixels). An important skill in Deep Learning is exploring your dataset, and understanding the format. Let's download MNIST, and explore it now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKvqxLsAo8fj",
        "colab_type": "code",
        "outputId": "4f816feb-a632-4c68-f109-9a9fb554e901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dataset = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = dataset.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II_QfaijpOrv",
        "colab_type": "text"
      },
      "source": [
        "There are 60,000 images in the training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0VgO9P6pQRw",
        "colab_type": "code",
        "outputId": "f336fe4d-14c6-4b54-e5f7-2c0c682156e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_images.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn3EcT0ppRG6",
        "colab_type": "text"
      },
      "source": [
        "And 10,000 in the testing set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VGb7BpFpSl9",
        "colab_type": "code",
        "outputId": "5cd5208f-d1f4-494e-89b2-41805ef87c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(test_images.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA9aCXKtpVs3",
        "colab_type": "text"
      },
      "source": [
        "Each label is an integer between 0-9:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "johPrDjepW9c",
        "colab_type": "code",
        "outputId": "3cd9eb13-cc6f-4a87-a849-40047b394135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5 0 4 ... 5 6 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gopt0n79JNTm",
        "colab_type": "code",
        "outputId": "7378a1ae-3b9b-4897-b59e-f998586864b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "print(train_images[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
            "  175  26 166 255 247 127   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
            "  225 172 253 242 195  64   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
            "   93  82  82  56  39   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
            "   25   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
            "  150  27   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
            "  253 187   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
            "  253 249  64   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
            "  253 207   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
            "  250 182   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
            "   78   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3M7RaIspX1j",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess the data\n",
        "The pixel values in the images range between 0 and 255. Let's normalize the values 0 and 1 by dividing all the images by 255. It's important that the training set and the testing set are preprocessed in the same way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PCe9-u4pg1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d28eIDCqpiVQ",
        "colab_type": "text"
      },
      "source": [
        "Let's display the first 25 images from the training set, and display the label below each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0GYYH_XpjuU",
        "colab_type": "code",
        "outputId": "6a25f2cc-9e88-4bb4-8fc7-ddf052ffe87d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(train_labels[i])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvfMnOtyqE7I",
        "colab_type": "text"
      },
      "source": [
        "## Create the layers\n",
        "\n",
        "Neural networks are made up of layers. Here, you'll define the layers, and assemble them into a model. We will start with a single Dense layer. \n",
        "\n",
        "### What does a layer do?\n",
        "\n",
        "The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. For example:\n",
        "\n",
        "- The first layer in a network might receives the pixel values as input. From these, it learns to detect edges (combinations of pixels). \n",
        "\n",
        "- The next layer in the network receives edges as input, and may learn to detect lines (combinations of edges). \n",
        "\n",
        "- If you added another layer, it might learn to detect shapes (combinations of edges).\n",
        "\n",
        "The \"Deep\" in \"Deep Learning\" refers to the depth of the network. Deeper networks can learn increasingly abstract patterns. Roughly, the width of a layer (in terms of the number of neurons) refers to the number of patterns it can learn of each type.\n",
        "\n",
        "Most of deep learning consists of chaining together simple layers. Most layers, such as [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), have parameters that are initialized randomly, then tuned (or learned) during training by gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta-amFtkr9DI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A linear model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc5N-mBCsJYS",
        "colab_type": "text"
      },
      "source": [
        "The first layer in this network, [tf.keras.layers.Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten), transforms the format of the images from a two-dimensional array (of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data. This is necessary since Dense layers require arrays as input.\n",
        "\n",
        "After the pixels are flattened, this model consists of a single Dense layer. This is a densely connected, or fully connected, neural layer. The Dense layer has 10 neurons with softmax activation. This returns an array of 10 probability scores that sum to 1. \n",
        "\n",
        "After classifying an image, each neuron will contains a score that indicates the probability that the current image belongs to one of the 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9npNSJYGsOs_",
        "colab_type": "text"
      },
      "source": [
        "## Compile the model\n",
        "\n",
        "Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n",
        "\n",
        "*Loss function* — This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
        "\n",
        "*Optimizer* — This is how the model is updated based on the data it sees and its loss function.\n",
        "\n",
        "*Metrics* — Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smPN2Oz1sP6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl710TxYsRN2",
        "colab_type": "text"
      },
      "source": [
        "## Train the model\n",
        "Training the neural network model requires the following steps:\n",
        "\n",
        "1. Feed the training data to the model. In this example, the training data is in the ```train_images``` and ```train_labels``` arrays.\n",
        "\n",
        "1. The model learns to associate images and labels.\n",
        "\n",
        "1. You ask the model to make predictions about a test set—in this example, the ```test_images``` array.\n",
        "\n",
        "1. Verify that the predictions match the labels from the ```test_labels``` array.\n",
        "\n",
        "To begin training, call the ```model.fit``` method — so called because it \"fits\" the model to the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snsjLq5vsTHv",
        "colab_type": "code",
        "outputId": "60ea9a4c-d4d3-4a2d-ba71-53bb0def19c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "EPOCHS=10\n",
        "model.fit(train_images, train_labels, epochs=EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2307 - accuracy: 0.9331\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0973 - accuracy: 0.9701\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0687 - accuracy: 0.9781\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0523 - accuracy: 0.9832\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0388 - accuracy: 0.9872\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0332 - accuracy: 0.9893\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0276 - accuracy: 0.9908\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0231 - accuracy: 0.9922\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0214 - accuracy: 0.9927\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0185 - accuracy: 0.9936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcb302b7518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT61pdPAsUgR",
        "colab_type": "text"
      },
      "source": [
        "As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.90 (or 90%) on the training data. Accuracy may be slightly different each time you run this code, since the parameters inside the Dense layer are randomly initialized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BCjtzzmse3t",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate accuracy\n",
        "Next, compare how the model performs on the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMAOvcHnsgOb",
        "colab_type": "code",
        "outputId": "83233f3d-a095-4a30-e2a2-07424c94097e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1022 - accuracy: 0.9775\n",
            "\n",
            "Test accuracy: 0.9775000214576721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpVIYwY_shqM",
        "colab_type": "text"
      },
      "source": [
        "It turns out that the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy represents overfitting. Overfitting is when a machine learning model performs worse on new, previously unseen inputs than on the training data. An overfitted model \"memorizes\" the training data—with less accuracy on testing data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vCw5qU4skSI",
        "colab_type": "text"
      },
      "source": [
        "## Make predictions\n",
        "With the model trained, you can use it to make predictions about some images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG3rn85Dslbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56liTfOzsnMa",
        "colab_type": "text"
      },
      "source": [
        "Here, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gNyhd-zsoYk",
        "colab_type": "code",
        "outputId": "6cc9cf1b-df16-45f6-c261-f827e6251728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(predictions[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0422675e-12 9.8313944e-09 2.7932190e-08 7.5272446e-09 2.8213562e-12\n",
            " 2.7194477e-12 6.7594983e-17 1.0000000e+00 1.6998712e-10 1.3435514e-08]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmJ884GFspZ_",
        "colab_type": "text"
      },
      "source": [
        "A prediction is an array of 10 numbers. They represent the model's \"confidence\" that the image corresponds to each of the 10 digits. You can see which label has the highest confidence value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oVXIHafsqsO",
        "colab_type": "code",
        "outputId": "1d347ae8-51fe-4409-95e7-b15793c4b45a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tf.argmax(predictions[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(7, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AE8NZjYsuSC",
        "colab_type": "text"
      },
      "source": [
        "# Exercise: Fashion MNIST\n",
        "\n",
        "In the above tutorial, you trained a linear model (a single Dense layer) on the MNIST dataset. As an exercise, let's modify your code above to:\n",
        "- Use a new dataset (Fashion MNIST)\n",
        "- Train a neural network (with two Dense layers, instead of just one)\n",
        "- Create plots to observe overfitting and underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kUMis2J3NT_",
        "colab_type": "text"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "You will need to make two changes in the code above.\n",
        "\n",
        "**1) Import the Fashion MNIST** \n",
        "\n",
        "To do so, change the line\n",
        "\n",
        "```\n",
        "dataset = keras.datasets.mnist\n",
        "``` \n",
        "\n",
        "to \n",
        "\n",
        "```\n",
        "dataset = keras.datasets.fashion_mnist\n",
        "```\n",
        "\n",
        "**2) Modify the model definition to create a neural network**\n",
        "\n",
        "To do so, change the lines from:\n",
        "\n",
        "```\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "to\n",
        "\n",
        "```\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "This will define a neural network with a single hidden layer. If you like, you can experiment by adding a third Dense layer, which will create a deep neural network. For example:\n",
        "\n",
        "```\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "After making the above changes, on the Colab menu   select *Edit -> Clear all outputs* and *Runtime -> Restart runtime* to restore this notebook to a clean state. Run the cells in the tutorial above to train your neural network on Fashion MNIST.\n",
        "\n",
        "**3) Add plots to observe overfitting**\n",
        "\n",
        "If trained for too long, a NN may begin to memorize the training data (rather than learning patterns that generalize to unseen data). This is called overfitting. Of all the hyperparmeters in the design of your network (the number and width of layers, the optimizer, etc) - the most important to set properly is ```epochs```. You will learn more about this in exercise two.\n",
        "\n",
        "To create plots to observe overfitting, modify your training loop as follows.\n",
        "\n",
        "Change:\n",
        "\n",
        "```\n",
        "history = model.fit(train_images, train_labels, epochs=EPOCHS)\n",
        "```\n",
        "\n",
        "to:\n",
        "\n",
        "```\n",
        "history = model.fit(train_images, train_labels, \n",
        "                    validation_data=(test_images, test_labels),\n",
        "                    epochs=EPOCHS)\n",
        "```\n",
        "\n",
        "This will capture the accuracy and loss on the training and validation data after epoch. To plot the results, create a new code cell, and add the following code:\n",
        "\n",
        "```\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(EPOCHS)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQjV6NPRaWYi",
        "colab_type": "text"
      },
      "source": [
        "# Game break: Teachable Machine\n",
        "If you'd like, now would be a good time to take a break from coding and try: https://teachablemachine.withgoogle.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0B8TOWwCsbM",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial: Cats and Dogs\n",
        "Your instructor will walk you through this section (please follow along and ask questions as you have them!). You'll train a CNN to classify images of cats and dogs using a real-world dataset you will download from the web."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob2E2ZIeSbDv",
        "colab_type": "text"
      },
      "source": [
        " ## Download and explore the dataset\n",
        "\n",
        "Although you are downloading large files, you are doing so in Colab through Google Cloud Platform (instead of over your local WiFi connection). This means that downloads will usually be fast, regardless of your internet connection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aPzoJnd_8QA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7EocqtTHnnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our dataset is a zip on the web\n",
        "origin = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=origin, extract=True)\n",
        "path_to_folder = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlL8BiAkLn1E",
        "colab_type": "text"
      },
      "source": [
        "The unzipped dataset has the following directory structure:\n",
        "\n",
        "<pre>\n",
        "<b>cats_and_dogs_filtered</b>\n",
        "|__ <b>train</b>\n",
        "    |______ <b>cats</b>: [cat.0.jpg, cat.1.jpg, cat.2.jpg ....]\n",
        "    |______ <b>dogs</b>: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]\n",
        "|__ <b>validation</b>\n",
        "    |______ <b>cats</b>: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ....]\n",
        "    |______ <b>dogs</b>: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrgB9RwpShf8",
        "colab_type": "text"
      },
      "source": [
        "The dataset is divided into train and validation splits. Let's create variables that point to each of these directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4b5Fsw2Lj-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dir = os.path.join(path_to_folder, 'train')\n",
        "validation_dir = os.path.join(path_to_folder, 'validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG7DjqVkLlCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj1dSkUlLs-m",
        "colab_type": "text"
      },
      "source": [
        "Now let's count the number of images in each directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfwja3aILv3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_cats_tr = len(os.listdir(train_cats_dir))\n",
        "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
        "\n",
        "num_cats_val = len(os.listdir(validation_cats_dir))\n",
        "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
        "\n",
        "total_train = num_cats_tr + num_dogs_tr\n",
        "total_val = num_cats_val + num_dogs_val\n",
        "\n",
        "print('Total training cat images:', num_cats_tr)\n",
        "print('Total training dog images:', num_dogs_tr)\n",
        "print('Total validation cat images:', num_cats_val)\n",
        "print('Total validation dog images:', num_dogs_val)\n",
        "print('---')\n",
        "print(\"Total training images:\", total_train)\n",
        "print(\"Total validation images:\", total_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbqGygJHvWd_",
        "colab_type": "text"
      },
      "source": [
        "You should see that we have 3,000 total images (2,000 in train and 1,000 in validation). Note that this dataset is balanced (we have an equal number of cats and dogs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6HxxIkqSsj6",
        "colab_type": "text"
      },
      "source": [
        "Tip: in addition to Python, you can run shell commands in Colab (for example, ```!ls $train_cats_dir```)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bOJXrOqSuoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls $train_cats_dir "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e_9iotCAH8E",
        "colab_type": "text"
      },
      "source": [
        "Let's display a couple images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAjuM662viPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGVX4U4RTeF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_ = plt.imshow(plt.imread(os.path.join(train_cats_dir, \"cat.0.jpg\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PM8Th5UAUOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_ = plt.imshow(plt.imread(os.path.join(train_cats_dir, \"cat.1.jpg\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYj7KnYUvj5v",
        "colab_type": "text"
      },
      "source": [
        "Note that the images are different sizes. Before feeding them into a CNN, we'll need to reshape them all to the same dimensions. We'll take care of that in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1A2CLadVnQa",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5JfskKZL716",
        "colab_type": "text"
      },
      "source": [
        "Next, we will need a way to read these images off disk, and to preprocess them. Specifically, we will need to:\n",
        "- Read the image off disk.\n",
        "- Decode contents of these images and convert them into RGB arrays.\n",
        "- Convert the pixels values from integer to floating point types.\n",
        "- Rescale the pixel from values between 0 and 255 to values between 0 and 1 (neural networks work better with small input values - under the hood, each input is multiplied by a weight, large inputs could result in overflow).\n",
        "\n",
        "Fortunately, all of these tasks can be done with the `ImageDataGenerator` class provided by `tf.keras`. It can read images from disk and preprocess them into proper arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbKVMUaBA14w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVOP4BOcL57E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's resize images to this size\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEYTv3G6L9I2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rescale the pixel values to range between 0 and 1\n",
        "train_generator = ImageDataGenerator(rescale=1./255)\n",
        "val_generator = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmM-kS68L-iY",
        "colab_type": "text"
      },
      "source": [
        "After defining the generators for training and validation images, the `flow_from_directory` method load images from the disk, applies rescaling, and resizes the images into the required dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff0NXGJEVPWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32 # Read a batch of 64 images at each step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zenaWVaHL_jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_gen = train_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                     directory=train_dir,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbgDJgcUMAeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data_gen = val_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                 directory=validation_dir,\n",
        "                                                 target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                 class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h77qZjn2MFaZ",
        "colab_type": "text"
      },
      "source": [
        "## Use the generators to display a few images and their labels\n",
        "\n",
        "Next, we will extract a batch of images from the training generator, then plot several of them with `matplotlib`. The `next` function returns a batch from the dataset. The return value of `next` function is in form of `(x_train, y_train)` where x_train is the pixel values and y_train is the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRkntEZ1MJev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_batch, labels_batch = next(train_data_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFItkxNsp1EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The shape will be (32, 150, 150, 3)\n",
        "# This means a list of 32 images, each of which is 150x150x3.\n",
        "# The 3 at the end refers to the R,G,B color channels.\n",
        "# A grayscale image would be (for example) 150x150x1\n",
        "print(image_batch.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6sb2tmtqEYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The shape (32,) means a list of 64 numbers\n",
        "# each of these will either be 0 or 1\n",
        "print(labels_batch.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4iqLgMjMMWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function will plot images returned by the generator\n",
        "# in a grid with 1 row and 5 columns\n",
        "def plot_images(images):\n",
        "  fig, axes = plt.subplots(1, 5, figsize=(10,10))\n",
        "  axes = axes.flatten()\n",
        "  for img, ax in zip(images, axes):\n",
        "      ax.imshow(img)\n",
        "      ax.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQUpgA1bMOti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_images(image_batch[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnj3jfpvTlih",
        "colab_type": "text"
      },
      "source": [
        "Next, let's retrieve the labels. All images will be labeled either 0 or 1, since this is a binary classification problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq1rm3jTTa9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here are the first 5 labels from the dataset\n",
        "# that correspond to the images above\n",
        "print(labels_batch[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuMd_qHITcxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here, we can see that \"0\" maps to cat,\n",
        "# and \"1\" maps to dog\n",
        "print(train_data_gen.class_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yukXvCdWMSDj",
        "colab_type": "text"
      },
      "source": [
        "## Create the model\n",
        "Your model will consist of three convolutional blocks followed by max pooling. There's a fully connected layer with 256 units on top. This model will output class probabilities (between 0 and 1) based on the `sigmoid` activation function. If the output is closer to 1, the image will be classified as a dog, otherwise a cat. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAvbO8Q2BEN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dd4nHoIMGy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, 3, padding='same', activation='relu', \n",
        "           input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN8hv9ItMXNr",
        "colab_type": "text"
      },
      "source": [
        "Compile the model, and select the adam optimizer for gradient descent, and binary cross entropy for the loss function (roughly, cross entropy is a way to measure the distance between the prediction we wanted the network to make, and the prediction it made)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sarE21oqMYgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB2GwR_EMZwm",
        "colab_type": "text"
      },
      "source": [
        "Let's look at a diagram of all the layers of the network using the `summary` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mgFrgh-MbL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGviR2YtBq_H",
        "colab_type": "text"
      },
      "source": [
        "This model has about 5M parameters (or weights) to learn. Our model is ready to go, and next we can train it using the data generators we created earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx6po3HfMcTu",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmEm3VVwMeRg",
        "colab_type": "text"
      },
      "source": [
        "Use the `fit` method to train the network. You will train the model for 15 epochs (an epoch is one \"sweep\" over the training set, where each image is used once to perform a round of gradient descent, and update the models parameters). This will take one to two minutes, so let's start it now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4ggEXVPWcxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe5gKJ6eMfNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSjVPgaZ1bLn",
        "colab_type": "text"
      },
      "source": [
        "Inside `model.fit`, TensorFlow uses gradient descent to find useful values for all the weights in the model. When you create the model, the weights are initialized randomly, then gradually improved over time. The data generator is used to load batches of data off disk. Then, for each batch:\n",
        "- The model performs a forward pass (the images are classified by the network).\n",
        "- Then, the model performs a backward pass (the error is computed, then each weight is slightly adjusted using gradient descent to improve the accuracy on the next iteration).\n",
        "\n",
        "Gradient descent is an iterative process. The longer you train the model, the more accurate it will become on the training set. But, the more likely it is to overfit! Meaning, the model will begin to memorize the training images, rather than learn patterns that enable it generalize to new images not included in the training set. \n",
        "\n",
        "- We can see whether overfitting is present by comparing the accuracy on the training and validation data.\n",
        "\n",
        "If you look at the accuracy figures reported above, you should see that training accuracy is over 90%, while validation accuracy is only around 70%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIYFh3m9MpvG",
        "colab_type": "text"
      },
      "source": [
        "## Create plots to check for overfitting\n",
        "Accuracy on the validation data is important: it helps you estimate how well our model is likely to work on new, unseen data in the future. To see how much overfitting is present (and when it occurs), we will create two plots, one for accuracy, and another for loss. Roughly, loss (or error) is the inverse of accuracy (lower is better). Unlike accuracy, loss takes the confidence of a prediction into account (a confidently wrong predicitions has a higher loss than one that is only slightly wrong)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1ZuxB7iMrBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeBooB9-bpht",
        "colab_type": "text"
      },
      "source": [
        "Overfitting occurs when the validation loss stops decreasing. In this case, that occurs around epoch 5 (give or take). Your results may be slightly different each time you run this code (since the weights are initialized randomly).\n",
        "\n",
        "Why does overfitting happen? When there are only a \"small\" number of training examples, the model sometimes learns from noises or unwanted details, to an extent that it negatively impacts the performance of the model on new examples. It means that the model will have a difficult time \"generalizing\" on a new dataset (making accurate predictions on images that weren't included in the training set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Tk_WFuauxw",
        "colab_type": "text"
      },
      "source": [
        "# Game break: Quick, Draw!\n",
        "If you'd like, now would be a good time to take a break from coding and try: https://quickdraw.withgoogle.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3hfnxDST1we",
        "colab_type": "text"
      },
      "source": [
        "# Exercise: Reduce overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-TD-ium4LJL",
        "colab_type": "text"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "In this exercise, you will use data augmentation and dropout to improve your model. Follow along by reading and running the code below. There are two **TODOs** for you to complete, and a solution is given below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFrRJObuMv8d",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation\n",
        "Overfitting occurs when there are a \"small\" number of training examples. One way to fix this problem is to increase the size of the training set, by gathering more data (the larger and more diverse the dataset, the better!)\n",
        "\n",
        "We can also use a technique called \"data augmentation\" to increase the size of the training set, by generating new examples from existing ones by applying random transformations (for example, rotation) that yield believable-looking images. \n",
        "\n",
        "This is especially effective when working with images. For example, our training set may only contain images of cats that are right side up. If our validation set contains images of cats that are upside down, our model may have trouble classifying them correctly. To help teach it that cats can appear in any orientation, we will randomly rotate images from our training set during training. This helps expose the model to more aspects of the data, and can lead to better generalization.\n",
        "\n",
        "Data augmentation is built into the ImageDataGenerator. You can specifiy different transformations, and it will take care of applying then during the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIWUljgzb5dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create new data generators, this time with \n",
        "# data augmentation enabled\n",
        "train_generator = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    rotation_range=45,\n",
        "                    width_shift_range=.15,\n",
        "                    height_shift_range=.15,\n",
        "                    horizontal_flip=True,\n",
        "                    zoom_range=0.5\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLl4-mjKb-vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_gen = train_generator.flow_from_directory(batch_size=32,\n",
        "                                                     directory=train_dir,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXaXALsZ5CST",
        "colab_type": "text"
      },
      "source": [
        "The next cell will show how the same training image appears when used with five different types of data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtLNwy9_b_RF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
        "plot_images(augmented_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsKX1NNpchmM",
        "colab_type": "text"
      },
      "source": [
        "We only apply data augmentation to the training examples, so our validation generator looks the same as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZjPRpJkchwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_generator = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQIyYxvAcknP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data_gen = val_generator.flow_from_directory(batch_size=32,\n",
        "                                                 directory=validation_dir,\n",
        "                                                 target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                 class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-s-DCjFcm1N",
        "colab_type": "text"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K34kcZfzco5M",
        "colab_type": "text"
      },
      "source": [
        "Another technique to reduce overfitting is to introduce dropout to the network. Dropout is a form of regularization that makes it more difficult for the network to memorize rare details (instead, it is forced to learn more general patterns).\n",
        "\n",
        "When you apply dropout to a layer it randomly drops out (set to zero) a number of activations during training. Dropout takes a fractional number as its input value, in the form such as 0.1, 0.2, 0.4, etc. This means dropping out 10%, 20% or 40% of the output units randomly from the applied layer.\n",
        "\n",
        "When appling 0.1 dropout to a certain layer, it randomly deactivates 10% of the output units in each training epoch.\n",
        "\n",
        "Create a new model using Dropout. You'll reuse the model definition from above, and add a Dropout layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW2NXkclDxo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3e7ir-HcnZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Your code here\n",
        "# Create a new CNN that takes advantage of Dropout.\n",
        "# 1) Reuse the model declared in tutorial above.\n",
        "# 2) Add a new line that says \"Dropout(0.2),\" immediately\n",
        "# before the line that says \"Flatten()\"."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hmzYnem6TEf",
        "colab_type": "text"
      },
      "source": [
        "## Solution\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnRSZBV86dy7",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "model = Sequential([\n",
        "    Conv2D(32, 3, padding='same', activation='relu', \n",
        "           input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Dropout(0.2),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMlmQwrwcu4V",
        "colab_type": "text"
      },
      "source": [
        "After introducing dropout to the network, compile your model and view the layers summary. You should see a Dropout layer right before flatten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L77Pt-WFcxYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEMGlbwWc254",
        "colab_type": "text"
      },
      "source": [
        "## Train your new model\n",
        "Add code to train your new model. Previously, we trained for 15 epochs. You will need to train this new modek for more epochs, as data augmentation and dropout make it more difficult for a CNN to memorize the training data (this is what we want!).\n",
        "\n",
        "Here, you'll train this model for 25 epochs. This may take a few minutes, and you may need to train it for longer to reach peak accuracy. If you like, you can continue experimenting with that at home."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jLoyPTRc2Va",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BnRJWB164DY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: your code here\n",
        "# Add code to call model.fit, using your new\n",
        "# data generators with image augmentation\n",
        "# For reference, see the \"Train the model\"\n",
        "# section above"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n0HeqPt6_-1",
        "colab_type": "text"
      },
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSwPqgkM7BU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7ktPjfbdCoC",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate your new model\n",
        "Finally, let's again create plots of accuracy and loss (we use these plots often in practice!) Now, compare the loss and accuracy curves for the training and validation data. Were you able to achieve a higher validation accuracy than before? Note that even this model will eventually overfit. To prevent that, we use a technique called early stopping (we stop training when the validation loss is no longer decreasing). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ma9lDmidMWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MsFn3y93rhS",
        "colab_type": "text"
      },
      "source": [
        "# Game break: Sketch-RNN\n",
        "If you'd like, now would be a good time to take a break from coding and try: https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3PePjvzimM_",
        "colab_type": "text"
      },
      "source": [
        "# Exercise: Flowers\n",
        "\n",
        "In this exercise, you write a CNN and use it to classify five different types of flowers (sunflowers, tulips, etc). The dataset contains 1000 images in the training set, and 500 in the validation set.\n",
        "\n",
        "You will download the dataset, read and preprocess the images using ImageDataGenerator, then create, train and evaluate a model. \n",
        "\n",
        "A code outline is written for you, and there are several sections for you to complete, using the same pattern as the tutorial above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjcR7Q4dirYe",
        "colab_type": "text"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgbUshXAipXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "origin = 'https://storage.googleapis.com/tensorflow-blog/datasets/mini_flowers.zip'\n",
        "path_to_zip = tf.keras.utils.get_file('mini_flowers.zip', origin=origin, extract=True)\n",
        "path_to_folder = os.path.join(os.path.dirname(path_to_zip))\n",
        "\n",
        "train_dir = os.path.join(path_to_folder, \"train/\")\n",
        "val_dir = os.path.join(path_to_folder, \"val/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbmKQQi0ium3",
        "colab_type": "text"
      },
      "source": [
        "### Read the images off disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPM16ncwiwFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_image_generator = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NdpFdHnixTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=32,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='categorical')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu4fgW7HiysA",
        "colab_type": "text"
      },
      "source": [
        "### Plot images and their labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en1MNIUuizze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_batch, labels_batch = next(train_data_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjDpMv6ni1II",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(image_batch[i])\n",
        "    plt.xlabel(str(labels_batch[i]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gGj5YfGi2W5",
        "colab_type": "text"
      },
      "source": [
        "## Understanding one-hot labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0s0FGfhi7Pv",
        "colab_type": "text"
      },
      "source": [
        "Notice the labels are in one-hot format. Let's add some code to display the class names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNyTCYGGi8dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_data_gen.class_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76-LJc28i9jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = {v:k for k,v in train_data_gen.class_indices.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8oA7cbqi32m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(image_batch[i])\n",
        "  plt.xlabel(class_names[tf.argmax(labels_batch[i]).numpy()])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0d-Epi9jAE4",
        "colab_type": "text"
      },
      "source": [
        "## Read the validation images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiJ56MFLjBKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Above, you created a ImageDataGenerator for the training set\n",
        "# Next, create one to read the validation images\n",
        "# For example:\n",
        "# validation_image_generator = ImageDataGenerator ...\n",
        "# val_data_gen = validation_image_generator.flow_from_directory ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lzfpQ0fjCic",
        "colab_type": "text"
      },
      "source": [
        "## Create a CNN\n",
        "\n",
        "Now, it's time to define your model. You can create a similar model to the CNN used in the tutorial above.\n",
        "\n",
        "The only difference is that the final Dense layer of your model (which classifies the data based on the features provided by the convolutional base) must use softmax activation and have five output classes:\n",
        "\n",
        "```model.add(Dense(5, activation='softmax'))```\n",
        "\n",
        "This is because we now have five different types of flowers, instead of just cats and dogs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dZRNMOZjDnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: your code here\n",
        "# Define a CNN using code similar to the above \n",
        "# For example\n",
        "# model = Sequential()\n",
        "# model.add ...\n",
        "# ...\n",
        "# The last line of your model should be:\n",
        "# model.add(Dense(5, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3glSdS-5jHaU",
        "colab_type": "text"
      },
      "source": [
        "After you have defined your model, compile it by uncommenting and running this code. Important: notice that the loss has changed to ```categorical_crossentropy```. This is necessary because the labels are in one-hot format. Finally, although these loss functions sound complicated, there are only a handful for you to learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCZxCpGAjIiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.compile(optimizer='adam',\n",
        "#              loss='categorical_crossentropy',\n",
        "#              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H3NBgimjKc0",
        "colab_type": "text"
      },
      "source": [
        "Now train your model for 10 epochs using ```model.fit```. If you like, you can try to create plots of the training and validation accuracy and loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFikmgCKjLqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: your code here\n",
        "# For example\n",
        "# model.fit ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Bxb1D9jM28",
        "colab_type": "text"
      },
      "source": [
        "If all has gone well, your model should be about 90% accurate on the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKDwUizojQdU",
        "colab_type": "text"
      },
      "source": [
        "## Solution\n",
        "\n",
        "``` \n",
        "# Read the validation images\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=32,\n",
        "                                                              directory=val_dir,\n",
        "                                                              shuffle=True,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='categorical')\n",
        "```\n",
        "\n",
        "```\n",
        "# Define a model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', \n",
        "                        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "```\n",
        "\n",
        "```\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    epochs=10,\n",
        "    validation_data=val_data_gen,\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5cpeEEprlOT",
        "colab_type": "text"
      },
      "source": [
        "# An advanced example: DeepDream\n",
        "If time remains, in this tutorial your instructor will walk you through a minimal version of DeepDream, an experiment to visualize some of the features a convolutional neural network has learned to detect. DeepDream is an advanced tutorial, and our goal is to introduce you to some of the fascinating (and unexpected) things you can explore with Deep Learning. \n",
        "\n",
        "Normally, when training a model we use gradient descent to minimize classification loss. In a CNN, this means we adjust the weights in the filters. In DeepDream, we start with a large, pretrained CNN (and leave the filters fixed!) We then use gradient descent to modify the input image to increasingly activate the filters. For example, if there is a filter that recognizes a certain kind of texture, we can progressively modify the image to contain more and more examples of that texture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ5DZa1urWkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzRZ1H47t20X",
        "colab_type": "text"
      },
      "source": [
        "## Download and display an image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-Ad_AGCrQvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7I2qKIvrUyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download(url, target_size=None):\n",
        "  name = url.split('/')[-1]\n",
        "  image_path = tf.keras.utils.get_file(name, origin=url)\n",
        "  return tf.keras.preprocessing.image.load_img(image_path, target_size)\n",
        "\n",
        "def show(img):\n",
        "  plt.figure(figsize=(8,8))\n",
        "  plt.grid(False)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "original_img = download(url, target_size=[225, 375])\n",
        "original_img = np.array(original_img)\n",
        "show(original_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNZadvf1t5Nx",
        "colab_type": "text"
      },
      "source": [
        "## Rescale the pixel values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHZtAv0YraD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(img):\n",
        "  \"\"\" Convert RGB values from [0, 255] to [-1, 1] \"\"\"\n",
        "  img = tf.cast(img, tf.float32)\n",
        "  img /= 128.0\n",
        "  img -= 1.\n",
        "  return img\n",
        "\n",
        "def unprocess(img):\n",
        "  \"\"\" Undo the preprocessing above \"\"\"\n",
        "  img = 255 * (img + 1.0) / 2.0\n",
        "  return tf.cast(img, tf.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD12eOvrt9wc",
        "colab_type": "text"
      },
      "source": [
        "## Import a large, pretrained CNN\n",
        "This model has been trained on ImageNet, a dataset with about 1M images in about 1K classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyNR_ayNrcRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base = tf.keras.applications.InceptionV3(weights='imagenet', \n",
        "                                              include_top=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVrd2zgMuPPm",
        "colab_type": "text"
      },
      "source": [
        "## Choose layers to activate\n",
        "Normally, when you train a neural network, you use gradient descent to adjust the weights to minimize loss, in order to accurately classify images. In DeepDream, the trick is to use gradient descent to adjust the **image**, in order to increasingly activate certain layers from the network. You can explore different layers and see how this affects the results. You can find all the layer names using ```model.summary()```. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRFucWpkreWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = ['mixed2', 'mixed3', 'mixed4', 'mixed5']\n",
        "layers = [conv_base.get_layer(name).output for name in names]\n",
        "model = tf.keras.Model(inputs=conv_base.input, outputs=layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULXM15ysuqhj",
        "colab_type": "text"
      },
      "source": [
        "## Custom loss function\n",
        "Normally, we would use cross-entropy loss (for classification), or mean squared error (for regression). Here, we'll write a loss function that describes how activated our layers were by the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAxeiGTCrgn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_loss(img):\n",
        "  img_batch = tf.expand_dims(img, axis=0)\n",
        "  layer_activations = model(img_batch)\n",
        "  losses = [tf.math.reduce_mean(act) for act in layer_activations]\n",
        "  return tf.reduce_sum(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2CRKNrau4Zh",
        "colab_type": "text"
      },
      "source": [
        "## Use gradient ascent to progressively activate the layers\n",
        "Normally, when training a model you use gradient *descent* to adjust the weights to reduce the loss. In DeepDream, you will use gradient *ascent* to maximize the activation of the layers you selected by modifying the image, while leaving the weights of the network fixed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B632smgOriZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def step(img, lr=0.001):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = calc_loss(img)\n",
        "\n",
        "  gradients = tape.gradient(loss, img)\n",
        "  gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
        "\n",
        "  # Because the gradients are in the same shape \n",
        "  # as the image, we can directly add them to it!\n",
        "  img.assign_add(gradients * lr)\n",
        "  img.assign(tf.clip_by_value(img, -1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnVu-oF0rlJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = tf.Variable(preprocess(original_img))\n",
        "\n",
        "steps = 1000\n",
        "for i in range(steps):\n",
        "  step(img)\n",
        "  if i % 200 == 0:\n",
        "    clear_output(wait=True)\n",
        "    print (\"Step {}\".format(i))\n",
        "    show(unprocess(img.numpy()))\n",
        "\n",
        "clear_output(wait=True)\n",
        "show(unprocess(img.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8vF9au2vDnp",
        "colab_type": "text"
      },
      "source": [
        "You can find a complete example on the [website](https://www.tensorflow.org/tutorials/generative/deepdream) (which includes additional code to generate less noisy images), and you may also be interested in exploring a related technique [Neural Style Transfer](https://www.tensorflow.org/tutorials/generative/style_transfer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxEnRYNS4rX6",
        "colab_type": "text"
      },
      "source": [
        "# Stay in touch\n",
        "\n",
        "- Twitter https://twitter.com/tensorflow\n",
        "- Blog http://blog.tensorflow.org/\n",
        "- YouTube https://www.youtube.com/tensorflow.\n",
        "\n",
        "Thank you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JztBOYqZ6yTm",
        "colab_type": "text"
      },
      "source": [
        "# Learning more"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfSulmXVQyJp",
        "colab_type": "text"
      },
      "source": [
        "Book recommendations\n",
        "* [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)\n",
        "* [Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n"
      ]
    }
  ]
}